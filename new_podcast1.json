{"podcast_details": {"podcast_title": "a16z Podcast", "audio_url": "https://pdst.fm/e/chtbl.com/track/85G57/cdn.simplecast.com/audio/3f86df7b-51c6-4101-88a2-550dba782de8/episodes/73e17e98-e1cb-4eb0-96bd-b9d61343cbb7/audio/dd04780f-e103-432a-ad20-e73eb2e56910/default_tc.mp3?aid=rss_feed&feed=JGE3yC0V", "episode_title": "AI Revolution: Disruption, Alignment, and Opportunity", "episode_image": "https://image.simplecastcdn.com/images/38c671cb-f233-4f8b-884e-e3c7bd47db16/6deeb595-a193-49cd-935a-29e8ba5d9784/3000x3000/the-future-podcast-fa.jpg?aid=rss_feed", "episode_transcript": " Experiences where creation amongst 65 or 70 million people is just part of the way it goes. You really want to think about, okay, what's somewhat possible today? What do you see glimpses of today? If this is like you have to fill out a thousand pages of paperwork and get 15 different licenses from different bodies to make an AI system, that's never going to work. Entertainment is like this two trillion dollar a year industry. And like the dirty secret is that entertainment is imaginary friends that don't know you exist. And I wouldn't bet against startups in a general sense there. It's so early. The AI revolution is here. But as we collectively try to navigate this game changing technology, there are still many questions that even the top builders in the world are grappling to answer. That is why A16Z recently brought together some of the most influential founders from OpenAI, Anthropic, CharacterAI, Roblox, and more to an exclusive event called AI Revolution in San Francisco. Today's episode continues our coverage of this event as we discuss the very real world impact of this revolution on industries ranging from gaming to design and the considerations around alignment along the way. Now, if you missed part one, do yourself a favor and que that up next so that you can eavesdrop on these top builders breaking down the current economics of this wave. Plus, whether scaling laws will continue and how these models will evolve to capture more of the world around us. Plus, if you'd like to listen to all the talks in full today, head on over to a16z.com slash AI Revolution. As a reminder, the content here is for informational purposes only, should not be taken as legal, business, tax, or investment advice, or be used to evaluate any investment or security, and is not directed at any investors or potential investors in any A16Z fund. Please note that A16Z and its affiliates may also maintain investments in the companies discussed in this podcast. For more details, including a link to our investments, please see a16z.com slash disclosures. As this wave continues to unfold, it is worth reflecting on just how wide-reaching it is. So in this episode, we start with Mira Murati, co-founder and CTO of OpenAI, explaining how she ended up focusing her career here of all places, especially after a degree in mechanical engineering and after working as an aerospace engineer. There's not going to be a more important technology that we all build than building intelligence. It's such a, it is such a core unit in the university, it affects everything. But in order for AI to impact everything, we'll need a lot of compute. The good news is that it's on the way. Here is Nam Shazir, co-founder of Character AI and lead author on the Seminole 2017 Transformer paper, calculating just how much compute will soon be available. I think I saw an article yesterday, like Nvidia is going to build like another one and a half million H100s like next year. So that's roughly a quarter of a trillion operations per second per person, which means that could be processing on the order of one word per second on a hundred billion parameter model for everyone on Earth. It's often not hard to convince people that this compute is coming or that it'll impact a wide variety of industries. But what can be hard to convince is that this disruption is a positive thing. But instead of pontificating, let's take a look back at what the democratization of technology has yielded in the past from people running platforms with millions of users. We'll start with Dylan Field, co-founder and CEO of Figma, commenting on how design has been shaped by technology for years. A16Z general partner, David George, coming in with the... Fiery question, is AI actually going to take the job of the designer in the future? You know, it's kind of interesting and the question is like, okay, will there be less things to design or is AI going to do all the design work? It's like you're on one of those paths, maybe on the first one of will there be less things to design? If you look at every technological shift or platform shift so far, it's resulted in more things to design. So you've got the printing press and then you have to figure out what you put on a page. And you've got even more recently mobile. You would think, okay, less pixels, less designers, right? But no, that's when we saw the biggest explosion of designers. And so maybe if you'd asked me this like beginning of the year, you might have said, okay, well, we'll all have these chat boxes and people will be asking questions in them. And that's going to be our interface for everything. You know, look at OpenAI. They're on a hiring acquisition spree trying to get product people and designers right now so that they're able to make great consumer products. It turns out design kind of matters. The second one of will AI be doing the design is I think pretty interesting. So far we're not there. Right now we're at a place where AI might be doing the first draft. And getting from first draft to final product actually turns out that's kind of hard and usually takes a team. But if you could get AI to start to suggest interface elements to people and do that in a way that actually makes sense, I think that could unlock a whole new era of design in terms of creating contextual designs, designs that are responsive to what the user's intent is at that moment. And I think that'd be a fascinating era for sort of all designers to be working in. But I don't think it replaces the need for human designers. So fewer pixels to design does not actually equate to fewer designers. And it turns out that many of the experiences where people are already spending hours a day have a lot of room for upside. Gnome here on how AI can drastically improve entertainment. Entertainment is like this two trillion dollar a year industry. And like the dirty secret is that entertainment is imaginary friends that don't know you exist. Like the reason people interact with TV or any of these other things, it's called like these parasocial relationships, like your relationship with TV characters or book characters or celebrities. And everybody does it. It's actually a cool first use case for a GI. Like essentially there was the option to like go into like lots of different sorts of applications and a lot of them have a lot of like overhead and requirements. Like you want to launch something that's a doctor. It's going to be a lot slower because you want to be really, really, really careful about not providing like false information. But friend you can do like really fast. Like it's just entertainment. It makes things up. That's a feature. And we likely won't build these fundamentally new experiences by dreaming them up in some lab. We'll get to great by iterating and putting these products into the hands of users. Here's Mira on how this approach underpinned chat GBT success thus far. We did make a strategic decision a couple of years ago to pursue product. And we did this because we thought it was actually crucial to figure out how to deploy these models in the real world. And it would not be possible to just, you know, sit in the lab and develop this thing in a vacuum without feedback from users from the real world. And also with chat GBT, you know, the week before we were worried that it wasn't good enough. And we put it out there and then people told us it is good enough to discover new use cases. And you see all these emergent use cases that I know you've written about. And that's what happens when you make this stuff accessible and easy to use and put it in the hands of everyone. There is beauty in putting such powerful tools into the hands of everyone. But how do we measure how powerful these tools are? Since 1950, people looked to the Turing test as one guidepost. But it turns out that the popular benchmark had its flaws. For one, it's surprisingly easy to trick humans. Now, as the AI community looks for new guideposts and benchmarks, here is David Bazucchi, co-founder and CEO of Roblox, proposing a, quote, new Turing test, vetting whether an AI can reason past the explicit data it's trained on. I have a Turing test question for AI, and that would be if we took AI in 1633 and trained on all the available information at that time, would it predict the Earth or the Sun is the center of the solar system? Even though 99.9% of the information is saying the Earth is the center of the solar system. So I think five years is right at the fringe of if we were to run that AI Turing test in the future. Interesting. Do you have a different answer if it was 10 years? 10 years, I think it'll say the Sun. Now, here's Dylan's version. What's the modern day Turing test? And I feel like this question kind of comes up everywhere now. And we're now seeing from these systems that it's easy to convince a human that you're human. It's hard to actually make good things. Yeah. Like, I could have GP4 create a business plan and come pitch you. That doesn't mean you're going to invest when you actually have two businesses side by side and they're competing and one of them is run by an AI and the other one is run by a human and you invest in the AI business, then I'm worried. Yeah. We're not there yet. Finally, here's Mira commenting on how OpenAI thinks about the threshold for AGI. How do you define AGI? In our OpenAI charter, we define it as a computer system, basically that is a system that is able to run on a single computer. We define it as a computer system, basically that is able to perform autonomously the majority of intellectual work. Passing the Turing test is one thing, but ensuring these models perform the goals that humans intend is another. Here, Mira shares how arguably the most successful AI product, Chatchipi-T, was born out of OpenAI trying to align the underlying model using reinforcement learning with human feedback. If you consider how Chatchipi-T was born, it was not born as a product that we wanted to put out there. In fact, the real roots of it go back to more than five years ago when we were thinking about how do you make these safe AI systems. You know, you don't necessarily want humans to actually write the goal functions because you don't want to use proxies for complex goal functions or you don't want to get it wrong. And so this is where reinforcement learning with human feedback was developed. What we were trying to really achieve was to align the AI system to human values and get it to receive human feedback. And based on that human feedback, it would be more likely to do the right thing, less likely to do the thing that you don't want it to do. Then after we developed GPT-3 and we put it out there in the API, this was the first time that we actually had safety research become practical into the real world. And this happened through instruction following models. So we used this method to basically take prompts from customers using the API. And then we had contractors generate feedback for the model to learn from. And we fine tuned the model on this data and built instruction following models that were much more likely to follow the intent of the user and to do the thing that you actually wanted to do. And so this was very powerful because AI safety was not just this theoretical concept that you sit around and you talk about, but it actually became, you know, was sort of like, how do you integrate this into the real world? And obviously with large language models, we see great representation of concepts, ideas of the real world. But on the output front, there are a lot of issues. And one of the biggest ones is obviously hallucinations. So how do you get these models to express uncertainty? And the precursor to ChildGPT was actually another project that we called WebGPT. And it used retrieval to be able to get information and site sources. So this project then eventually turned into ChildGPT because we thought the dialogue was really special because it allows you to sort of ask questions, to correct the other person, to express uncertainty. There's just so much. You can get down the error because you're interacting. Exactly. There is this interaction and you can get to a deeper truth. We started going down this path and at the time we were doing this with GPT-3 and then GPT-3.5. But, you know, one thing that people forget is that actually at this time we had already trained GPT-4. And so internally at OpenAI, we were very excited about GPT-4 and sort of put ChildGPT in the rear view mirror. And we kind of realized, OK, we're going to take six months to focus on alignment and safety of GPT-4. And we started thinking about things that we could do. And one of the main things was actually to put ChildGPT in the hands of researchers out there that could give us feedback since we had this dialogue modality. And so this was the original intent to actually get feedback from researchers and use it to make GPT-4 more aligned and safer and more robust, more reliable and eventually plan. I mean, just for clarity, when you say align and safety, do you include in that like correct and does what it wants? Or do you mean actual like protecting from some sort of harm? By alignment, I generally mean that it aligns with the user's intent. So it does exactly the thing that you want it to do. But safety includes other things as well, like misuse, where the user is intentionally trying to use the model to create harmful outputs. In this case with ChildGPT, we were actually trying to make the model more likely to do the thing that you want it to do, to make it more aligned. And we also wanted to figure out the issue of hallucinations, which is obviously an extremely hard problem. But I do think that with this method of reinforcement learning with human feedback, maybe that is all we need if we push this hard enough. Given that this field is so early, so are the methods of alignment. Here's another approach that Dario from Anthropic has proposed, one that involves a guiding constitution and AI that reinforces those principles. Here's Dario in conversation with A16Z general partner Anjani Minha. The method that's been kind of dominant for steering the values and the outputs of AI systems up until recently has been RL from human feedback. I was one of the co-inventors of that at OpenAI, but since then it's been improved to power CHATGPT. And the way that method works is that humans give feedback on model outputs, say which model outputs they like better. And over time, the model learns what the humans want and learns to emulate what the humans want. Constitutional AI, you can think of it as the AI itself giving the feedback. So instead of human raters, you have a set of principles. And our set of principles is in our constitution. It's very short. It's five pages. We're constantly updating it. There could be different constitutions for different use cases, but this is where we're starting from. And whenever you train the model, you simply have the AI system, read the constitution, look at some task, like, you know, summarize this content or give your opinion on X. And the AI system will complete the task. And then you have another copy of the AI system say, OK, was this in line with the constitution or was it not? At the end of this, if you train it, the hope is that the model acts in line with this guide star set of principles. So as a result of that approach, you know, the seed of the constitution captures some set of values of the constitutional authors. Right. How are you grappling with the debate that that means you are imposing your values on the constitutional system? Yeah, a couple of directions in that. So first, when we took the original constitution, you know, we tried to add as little of our own content as possible. We added things from like the UN Declaration on Human Rights, just kind of like generally agreed upon kind of, you know, deliberative principles, some"}, "podcast_summary": "The podcast episode discusses the impact of AI revolution on various industries and the considerations around alignment in AI development. It features discussions with influential founders and leaders in the AI space. Topics include the democratization of technology, the role of AI in design, the potential of AI in entertainment, measuring the power of AI, AI models' ability to reason past explicit data, the modern-day Turing test, and OpenAI's approach to alignment and safety. Other approaches to alignment, such as constitutional AI, are also explored.", "podcast_guest": "Mira Murati", "podcast_guest_org": "OpenAI", "podcast_guest_title": null, "podcast_guest_info": "Not Available", "podcast_guest_url": null, "podcast_highlights": "\"The reason people interact with TV or any of these other things, it's called like these parasocial relationships, like your relationship with TV characters or book characters or celebrities. And everybody does it. It's actually a cool first use case for a GI. Like essentially there was the option to go into like lots of different sorts of applications and a lot of them have like a lot of overhead and requirements. Like you want to launch something that's a doctor. It's gonna be a lot slower because you want to be really, really, really careful about not providing false information. But Friend, you can do really fast. Like it's just entertainment. It makes things up. That's a feature.\" - Gnome, Co-Founder and CEO of Roblox [source: AI Revolution podcast by A16Z]", "podcast_hashtags": ["podcast", "AI", "revolution", "technology", "design", "alignment"]}