{"podcast_details": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "audio_url": "https://chrt.fm/track/4D4ED/traffic.megaphone.fm/MLN9429073065.mp3?updated=1695659369", "episode_title": "Personalization for Text-to-Image Generative AI with Nataniel Ruiz - #648", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " All right, everyone. Welcome to another episode of the TwiML AI Podcast. I am your host, Sam Charrington, and today I'm joined by Nathaniel Ruiz. Nathaniel is a research scientist at Google. Before we get going, be sure to take a moment to hit that subscribe button wherever you're listening to today's show. Nathaniel, welcome back to the podcast. Thank you so much. Yeah, it's been a while. It's been like three years, maybe. It has been about three years, maybe a little bit more, and I'm really looking forward to chatting with you about what you're working on nowadays, which is personalization for generative AI models. Before we jump into that, I'd love to have you share a little bit about your background with our audience. Absolutely. So yeah, a lot has happened since then. A lot of my research topics have changed, but I think for very interesting directions. My background is I was born in Bolivia. I was 18, I left for France, and I did my undergrad there in computer science in Paris at Equipari Technique. And then I came to the US, I did a master's at Georgia Tech, and then I did my PhD at Boston University, which I just finished in March this year. And then I joined Google right after. So I was doing a long internship at Google in my last year, where we did a lot of the work that kind of set me on this path right now. And then yeah, I joined Google full-time as a research scientist, so I'm working there right now. Congrats. Thank you. We were talking about deepfakes and adversarial attacks, and that was part of your graduate work. Are you still working on those topics? Yeah, funnily enough, it didn't even make it into my thesis. My thesis ended up being about simulating images and video in order to train and test machine learning models in efficient ways. That was kind of like the topic. So I tried to make it very kind of related, all of the papers that went into that. So the deepfake and adversarial attack against deepfakes work didn't really kind of relate as much into that topic. But I did some cool work there. I think we were a little bit early with that idea, which the kind of like core idea is to use an adversarial attack on images in order for people that are trying to like kind of generate deepfakes out of those images to basically thwart them from generating deepfakes. And so it's basically an adversarial attack. Like the most basic idea is to do an adversarial attack against a conditional image translation network in order for the network to completely fail when used on this specific input. And in that way, kind of we were a little bit early because the methods that were going around then and there didn't work very well with very few images. The outputs were in that realistic. And I think that's kind of had a bit of a revival now with all of the incredible like generative models for images and video that have been popping up. So this is actually kind of taking like a little bit of a renaissance, that idea. And now there's like a bunch of work. Like for us, it was a bit hard to get this published or like understood at the time. And now it's like very, I think, easy for people to understand it when they see the quality of the generations now. So they understand the value of needing to protect identity and from being replicated, etc. So there's a lot of work like from Alexander Madri's lab at MIT that has been kind of reviving this issue, which is really cool, really important, I think. Interesting. So your focus today is on, broadly speaking, as I mentioned, personalization for generative AI models. Talk a little bit about just the problems you're exploring and trying to solve there. Yeah, so kind of chronologically how this happened, how I got interested in, I guess, is like I had work at Apple that's called MorphGAN, which was basically, this is kind of what set me on the deep fake disruption route with adversarial attacks, because we realized that if you train a model on, like this was again at the time, this was like 2019, I think, you train a model on a lot of images, like paired images of faces, then you're able to kind of like manipulate these faces and generate like one shot, kind of deep fakes of people. So it was pretty cool in order to like, you know, have your avatar and be able to like talk, but obviously has some repercussions with respect to privacy that set me into that route. So that's where I had some experience in generative models and then kind of got recruited here at Google for an internship on personalizing generative models in order to like generate objects, for example, like you want to generate this specific can in like different scenarios, different contexts, and you only have a limited amount of images of that can or this dog or something. So that's when we set out to do that. But this was right at the moment where Dali2 had come out and Imagine had come out from Google. So we got early access to Imagine internally, and we were able to kind of set out that route of like personalization for generative models at the time for diffusion models that were generating such good quality images. And surprisingly, you know, simple techniques work incredibly well with diffusion models. And that's kind of how we came up with Dream Booth, which is basically personalizing a diffusion model or a generative model for a specific subject in order to generalize it in different contexts and situations. And then we discovered they could do different styles, etc. And so that kind of set out my current direction, which is personalization of generative models. Is Dream Booth, you know, it's the beginning of the title of the paper of that name. But is it also a product? Is it a model? Is it a system? You know, what all is Dream Booth? Yeah, I would say Dream Booth is kind of like a method or an algorithm. That's what I would say. And it's like the idea is personalization of a generative model with few input images. The personalization of a generative model using few images of a subject for the subject-driven generation. I think that's the term we came up with, subject-driven generation. Which is basically you want to grab the model and you want to query it in different ways. And then you want to generate novel pictures of that subject. So it could be like a cat or, you know, like some type of... So I upload a few pictures of my cat or my dog and say, okay, now show me Sparky in Paris at night with the orange sky or something like that. And the idea is that this algorithm will generate images based on that prompt with that subject. Exactly. I mean, I think you explained it very well. So that's like... It was kind of like a new problem because methods at the time weren't able to solve this problem before. And it was kind of a problem because the capabilities of current models weren't there yet. And then when the models had come out, like the diffusion models like Dali2 and Imagine, then it was kind of our contribution there is to create a method that is able to kind of tackle this problem. So like formalize this problem, tackle this problem. And also in the paper, we have a lot of scientific contributions such as like, how do you measure success in this scenario? And we created the first data set that is large for this problem. So we kind of set out a new direction for research in this area. Are there things that, for folks that are tangentially familiar with diffusion models but not familiar with their details, are there elements of the... The details, the workings of diffusion models that are important to understand, to kind of fully understand what you've done and the way that you've used them? So diffusion models kind of like, I think I guess already existed like about seven years ago, I think. We were in 2016, if I'm correct. I'm not 100% sure. But then like in 2019, I think there were a bunch of papers that really made them work very well. And then 2019, 2020, and then more and more like training on larger data sets, getting some of the techniques right. And then they were kind of... Now they've superseded like GANs for the best generative models. The core idea is you're basically learning to denoise images. So I guess a very natural application scenario was to do denoising or super resolution as applications with this, which basically the core idea of the diffusion model is you grab an image that's clean, you noise it intermediately with a specific amount of noise, and then you train a model that tries to denoise that image in one step. And then in inference, you go from... If you want to generate new images, like people realize that you could generate new images after you train this model if you just grab a full noise, so full noisy image, and then you can go fully into the clean image regime iteratively denoising that image. And that actually works really well, which is kind of like it's an interesting kind of concept because it's an asymmetric thing where your training is different from your inference, but it works extremely well. One thing that really makes it work is the new techniques that have been developed inside of Google and OpenAI and Berkeley in the last three to four years. And there's a bunch of papers on that. And then one of the things that really makes them amazing is that they're much more stable than GANs. They're easier to train basically, because GANs, you have a lot of stability issues. You can have mode collapse. You have to kind of be an expert in order to get your GAN to train well. And a lot of people are trying to do that. But I think with diffusion models, you have much more flexibility in what you're doing with the architecture, what you're doing with certain things, and it's kind of easier to get them to work. And that can change everything. I also think it's objectively a really good idea, so that's why it works. Is it fair to think about the way you set up the problem as the traditional way of doing image generation, as you are kind of conditioning the image generation on some texts? Do you think of the subject-driven generation as additional conditioning or a constraint or something else? Like, does that question make sense? Yeah, it makes a lot of sense. I think you can think of conditioning the whole procedure of generating new images for this kind of cat or something as, yeah, conditioned on the data set. But in practice, what we do is we don't kind of create a new conditioning pipeline in the network. What we just do is we just do fine-tuning on these images, basically. So that's how, like, technically, this is how it happens. So I wouldn't call it, like, yeah, conditioning per se, but more recent models are trying to do faster DreamBooth, and they do take this conditioning route, so conditioning on a small data set of images. What we did, which is the contribution that we have for DreamBooth, is like, okay, you have the set of images of the cat, then what happens if you fine-tune them? Like, okay, so we figured out a way to fine-tune these diffusion models on this set of images of this cat by just adding, like, a personalized or a personal token that is rare that kind of identifies this cat. So it can be, like, a string of characters that doesn't have a strong prior in the model. And we condition, like, these. So we train the model using this kind of small data set of cat images with prompts A, V, where the V is, like, the rare identifier, and then the name, so the class name cat. So the model can localize, you know, spatially the subject of interest, and that works very well actually. And if you do early stopping with this very simple technique of fine-tuning the model on this small data set with these prompts that we came up with, then it works really, really well. And you can start generating the cat. It doesn't overfit because you do some early stopping, and it was able to, like, memorize this subject. So then it can, like, regenerate in different, even sometimes different poses and, you know, with different lighting and different circumstances and also, like, different styles. So a lot of very surprising things that we get out of these diffusion models. You make it sound very simple. So yeah, I guess, honestly, I love that about it. I think it's something that I think we were kind of first to see that this was possible with diffusion models. And because of several things probably, like, we don't have, like, a clear answer on why this simple algorithm works. But you know, possibly it's because these models are, like, huge, you know, like, much larger than previous models. They're also trained on much more data. It's trained on pairs of text and images. So a lot of those reasons and also, like, some properties of diffusion models can, like, help with avoiding memorization. Because if you try to train again on very few samples, then you get mode collapse very easily. So you start generating the exact same samples very quickly. With diffusion model, this happens more slowly. And it can have something to do with kind of maybe how the training is constructed, where you kind of train diffusion model to denoise partially-noised images. And maybe that makes it, you know, overfit less fast, you know. So those are all hypotheses on why it works. But we kind of were the ones that figured out, you know, oh, this works and this super simple idea works. We don't even have to fine-tune a subset of the parameters, find which parameters to fine-tune. We can just fine-tune the whole model on this very small dataset with this very simple caption and it works. So I really love that the idea is simple. And it has, like, I think that's where it really exploded because people could use it pretty easily or implement it pretty easily. And it just worked. You know, what I would have imagined to happen is, you know, less about overfitting and more about the generated images being kind of reminiscent of the subject, but not really fully capturing the subject. But the samples that you show, like the subject is there and it doesn't seem like it's distorted. It does a really good job of recreating the subject in the scenario. And granted that you just said that, you know, we don't understand fully why this works, all that kind of stuff. Any thoughts on why that part works? Why the subject is recreated? So, you know, carefully yet without the overfitting. Yeah, like the subject details are very well preserved. I think for the one, the model that we were using, Imagine at the time, it was a great model trained on a very large dataset. So I think what it can do is, like, it already has like a concept of, for example, for dogs, it was pretty easy to train it on dogs because it already has concept of these, you know, specific dogs. So it can mix its prior with the details that you're trying to teach it, which are a specific kind of like details in the fur or colors or shapes, et cetera, of that dog. So it's kind of leverages is very strong prior in order to learn these things. And for like pixel level diffusion models, which is basically you don't have any kind of like down sampling until late in space at all. I think there is no real issue because you could overfit to a specific image is very easily. You just like train it all a long time on an image. So it will learn how to generate almost anything you give it, especially if it has this like already pre trained like it's a large pre trained model. So it will be able to memorize things much faster than a random model, I think. And for the latent case for the, for example, like stable diffusion, which is a latent diffusion model, then the only kind of obstacle that I see it from like actually reproducing the subject with good fidelity would be the encoder, the auto encoder. And the auto encoder is able to kind of reproduce most images with pretty high fidelity. It's not perfect, but it's able to do it. And then with SDXL actually, it is like much better auto encoder so it can reproduce many, you know, most images alm"}, "podcast_summary": "The podcast episode features an interview with Nathaniel Ruiz, a research scientist at Google. Nathaniel discusses his background, including his previous work on deepfakes and adversarial attacks, and his current focus on personalization for generative AI models. He explains the concept of diffusion models and how they work by training a model to denoise images. Nathaniel introduces Dream Booth, a method he developed for personalizing generative models using a small set of input images. He describes how Dream Booth fine-tunes diffusion models to generate novel images based on a specific subject, such as a cat or a dog. Nathaniel also discusses the simplicity of the Dream Booth algorithm and its ability to preserve subject details without overfitting. He mentions possible reasons for its success, including the use of large pre-trained models and the effectiveness of the autoencoder. Overall, the podcast provides insights into the exciting field of personalization for generative AI models and the potential applications of Dream Booth.", "podcast_guest": "Nathaniel Ruiz", "podcast_guest_org": "Google", "podcast_guest_title": null, "podcast_guest_info": "Not Available", "podcast_guest_url": null, "podcast_highlights": "\"Personally, I think Dream Booth is a fascinating algorithm that allows for the personalization of generative models using a small set of input images. It's a simple yet powerful idea that has shown impressive results, particularly in recreating the subject with remarkable fidelity. The technique leverages the capabilities of diffusion models and fine-tuning on a personalized dataset to generate novel images based on specific prompts. It's a highly accessible method that has gained a lot of attention in the research community. If you're interested in learning more about Dream Booth and its applications, I recommend checking out the paper by Nathaniel Ruiz et al. titled 'Dreaming to Personalize: Subject-Driven Image Generation with Few Shots'.\" (Source: TwiML AI Podcast - Episode 460)", "podcast_hashtags": ["generative", "AI", "models", "personalization", "deepfakes", "adversarial", "attacks", "diffusion", "models"]}