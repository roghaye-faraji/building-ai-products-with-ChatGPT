{"podcast_details": {"podcast_title": "The AI in Business Podcast", "audio_url": "https://chtbl.com/track/D8572/traffic.libsyn.com/secure/techemergence/AI_In_Business-_Gordon_Wilson_-_9.30.23.mp3?dest-id=151434", "episode_title": "[Beyond GPU] Finding ROI for AI at the Edge - with Gordon Wilson of Rain", "episode_image": "https://ssl-static.libsyn.com/p/assets/1/c/0/2/1c02772256d769adbafc7308ab683e82/The_AI_in_business_new_Blue_32x.jpeg", "episode_transcript": " Welcome everyone to the AI in Business podcast. I'm Matthew Demello, Senior Editor here at Emerge Technology Research. Today's episode is the first in a special series we're calling Beyond GPU, taking a look at edge AI computing challenges and solutions with help from guests at leading vendors and superscale global tech brands leading the most advanced hardware platform teams on the planet. Our first guest in the series is Gordon Wilson, CEO of RAIN. RAIN is a technology company that builds integrated hardware software platforms for radically cheaper AI solutions, which they call artificial brains. Gordon joins the program today to discuss business problems facing companies across industries regarding driving edge computing power, offering best strategies for where business leaders can find value. Later, he pulls apart compelling use cases across industrial robotics and smart retail. Without further ado, here's our conversation. Gordon, thanks so much for being with us on the program. Thank you so much, Matt. It's great to be here. Absolutely. So RAIN is very special, if I may say so myself. And I think you guys occupy a very unique place, both in terms of infrastructure and where the rubber hits the road in terms of the capabilities. Could you tell us a little bit more about RAIN and what you're doing to help our audience understand how we're going to color our answers from here on in the program? Absolutely. So we're RAIN AI and our mission is to make AI radically cheaper. AI today is extraordinarily expensive and it is prohibitive in what we can realize, whether it's in the hyperscale cloud or in the far edge. And we are really focused on solving the most fundamental problems in artificial intelligence. How do you make both inference and training dramatically cheaper by fundamentally reimagining the hardware and the algorithms that support all of the cutting edge AI models today? So our first products will be edge AI processors. So we're focusing on the edge first. And importantly, we expect these to be about 100x more efficient than the incumbent processors that you may see available today on the market. And importantly, all of our products, and this is pretty unique, particularly at the edge, all of our products are built for integrated inference and training. There is so much value to be unlocked by deploying inference and most folks who are thinking about deploying AI to the edge consider simply inference. But it's not just inference that we want to solve and move and put in all types of machines. We also want machines to be able to learn on their own, to be able to adapt, to be able to be personalized, have secure and private data. So we're making machines be able to do inference and training simultaneously. But a little more about zooming out at RAINN as a company and as a philosophy, I look at a lot of other AI chips being developed out there and they're mostly being developed by folks who have and who, and rightfully, should have deep understanding of the semiconductor world. We have that expertise in our team, but we're really an AI-first company. We started by spending years studying the fundamentals of AI algorithms and emerging hardware substrates. We've developed this expertise and know-how to really, we don't want to build the infrastructure for just AI today, we want to build it for the future. So a few things that I want to emphasize before we dive into the technical products and the business use cases, we're pretty unique as an AI infrastructure company in that we've had some collaborations and some support from folks that are really at the frontier of AI research. We've published papers with Yoshua Bengio on advanced AI algorithms, integrating those with hardware. If you listen closely to Jeff Hinton when he is on podcasts or in his recent papers, we're the only hardware company that he refers to as really thinking about the frontier. When he talks about his mortal computation idea, he has referred to us both in his papers and on podcasts and things like that. We're the only AI chip company that Sam Altman, the CEO of OpenAI, has led an investment round in. I don't say this just to pitch ourselves, but we're really positioning ourselves and have positioned ourselves over the last six years to align with the thought leaders at the frontiers of artificial intelligence. We know that what we're building is for what, not only for today, but what comes next. Yes, absolutely. If you see Yoshua send him our regards, he is a friend of the show. I think Dan just had him on a couple of weeks ago. Small community, right? It's not that much of bragging. He's in the room somewhere. Anyway, but diving into it, you talked a lot about inference in your last answer. I'm hoping we can roll out the red carpet for the audience just in terms of putting a finer point on what we mean by edge AI. I'm hoping also in your answer, we can really stress maybe the role that inference is going to play in there and become more part of the language as edge AI use cases become more mainstream in our conversations about where these capabilities are going. Absolutely. No, AI inference is the deployment of models to make decisions about rich information that they can receive at the location of that information. Today, the best AI that's been developed is cloud-based. We have massive, massive data centers that have thousands of GPUs. This is where the vast majority of capital that's been invested this year, particularly with the onset of things like Jatch-EBT, has been allocated towards. Rightfully so, because when you have the scale of data center compute, you can solve some really hard problems really by throwing brute force of computation at them. The problem is that that approach to building AI is so expensive that it's stuck. It's stuck in those data centers, and cloud-based AI really just falls short when you want to deploy AI en masse. That deployment is, of course, with straightforward inference, but I would also, again, reiterate it's not just inference, it's also training. But inference alone to the ability to make those decisions on the fly at the location of where that information is received at the cell phone, at the drone, in the robot, at the car, this is critical that it is very fast, that it is very power efficient, and it is just as performant, as accurate, as intelligent as possible. So we don't just want AI in our data centers running from the cloud. We want it everywhere. Cloud-based AI, again, it has very high operational expenses. There's high network and latency bandwidth requirements. It makes real-time processing of real-time information just pretty much impossible. So you need to rethink infrastructure if you're going to deploy AI into low-power devices, and you need to build something that is not only low-power, but is really, really performant. And so far, in the market today, we've seen some chips that are really low-power but are kind of dumb that don't really have a lot of capabilities. Maybe it's just recognizing a keyword, or it's like a binary recognition. And we have other processors that are smarter, but the edge for them is like 40 watts, 50 watts, 60 watts. These are not chips that could fit inside of a cell phone. So we need something different, something that hits both of those marks. That's very high performance that can do really powerful, or first targeting computer vision. So really powerful computer vision, inference and training, and we want to be able to fit that into basically a one watt footprint. That is the first product that we intend to bring to market. An incredibly performant one watt chip that can do all of the classic computer vision models for image recognition, object segmentation, video super resolution, within that power footprint, and not just do inference, but also do training. So just in terms of that first product you're talking about, this seems like the keyhole everybody's got to go through here in order to get to the next level. Once we do have this product, and it sounds like you're on the way to making it if you haven't already, what problems will we be able to solve? I know you were kind of honing in on a few use cases that are going to blossom, but I guess maybe more to the point in terms of industry, what problems are going to become a thing of the past? Absolutely. So we want to embed intelligence into every type of machine. And we're building a chip at a power footprint that is just not available today. That power performance era is not possible today. So what does that mean in terms of use cases? We've seen a lot of advancement of drones and robotics in the last few years. And I think a lot of people have recognized that many of the mechanical engineering challenges in terms of building robots, the actuators, the robotic arms, these are solved problems. And we have great engines and maneuverability of drones. But the intelligence that we can put inside of this is extremely limited because of the cost of that intelligence. So we want to make drones and robots much more intelligent and much more capable to make better decisions on their own. What does that really mean? Let's say you have a drone that is there to deliver resources to a very remote space. There's a very, very great company that is focusing on this problem. This type of drone may be trained to fly in clear skies. We want this drone to, even if it's trained to fly in clear skies, that when it starts raining, it can learn and adapt to still deliver that package to its destination with extremely high accuracy. That's one of the capabilities of online learning that we're making possible. We have today the complexity of or the dimensionality of information that it can receive. For instance, in the resolution of the image is limited because of also compute. So we would like a drone to have a 4K camera that has extremely high resolution of input and be able to have ultra-fast recognition across all frames of that image, all segments of that image in real time. Today, drones and robots are limited. We can't build C3PO. This first product won't make C3PO possible, but it's going to make these things far, far more responsive. It'll make them faster. It'll allow them to adapt to changing conditions. It'll simply mean that all of these things, whether it's delivering goods to people or inspecting our infrastructure or any of the types of use cases that we see or even potentially in defense, it'll make these far, far more robust to the variety of conditions that they may operate in. Drones and robotics is one. Another space that we're very, very excited about, one that requires really high-power computer vision and is critically limited by both latency and power is augmented reality and virtual reality, extended reality. You know, this, we've seen the release of the Apple headsets. We've seen Meta and the Quest headsets. There has been so much excitement around virtual reality and augmented reality for so long, but something that hasn't been necessarily part of the conversation but has always been there is that the key limiting factor for making these machines really useful and also fun and not nauseating to use is AI compute. These systems have often dedicated chips that have been built to support gesture recognition, eye tracking recognition. These are all ML models that are very, very compute intensive. And even today, the best in class requires you to have a battery pack in your back pocket and latency is still slow enough that it can make some people sick. We want to have a chip that can have so fast and so reliable gesture recognition, eye tracking, that it's truly a seamless experience for extended reality. That is something that's not possible today, but our product might make possible. And I think one of the biggest things that, one of the biggest markets that I think about is really mobile phones. They, there isn't an edge AI processor available that has, is in the single wattage footprint range that can deliver this type of computer vision. And every phone today, every phone has a camera, every phone is taking in visual information. There are so many applications, so many use cases that could be enabled if we could embed this intelligence into every mobile phone in the world, whether it's about recognizing objects to helping people navigate their world, recognizing the plants in their yard, or creating extended reality experiences or augmented reality experiences through their phone, through creating more advanced security protocols. There are billions of devices in the world today that have access to rich visual information, but they can't act on it. We want to make that possible. Yeah. And I think that's a moment, not unlike what we're seeing with generative AI. And no, no one really could have, could have predicted that. And I was having a conversation earlier this week that the gen AI explosion almost seems bigger than the rest of AI. It really feels like an AD, like a BCAD moment, even more than just like the bare bones technology, or even the rise of social media, which I think was a lot of people's first interaction with, with, with real artificial intelligence, whether they kind of knew it or not about like, you know, 12, 15 years ago. But in terms of, and we say with a lot of the times I'm in the conversations we're having with generative AI, the current moment feels a lot like the dawn of the internet. You know, everybody's used to these norms, you know, oh, oh, isn't it amazing that you're, you're the images on a webpage open in 20 seconds. You know, that was amazing in 1998. And by the year 2000, maybe two 2004, it was like, listen, everybody on my street has, has great ethernet. And if my website takes more than two seconds to load, that is, that is a travesty. So I'm almost wondering, you know, especially where you talk about the example of every, every mobile phone and, you know, this edge capability really getting in everybody's hands. That still sounds like a transformative moment. I hate to try to like, I don't know, predict a superstar or like, you know, predict another open AI moment or, or chat GPT moment, but that does sound incendiary. What other glimpses are we getting into the future, especially as this technology really starts to hit the ground floor and my mom starts to use it? Yeah, no, it's, it's just more responsive environments, you know, more personalized experiences. And when you can have training, the learning in these machines as well, it means that personalization can be private as well. It can be secure. People can engage with artificial intelligence. These agents can get to know them and we can have the, the peace of mind that it's just getting to know us and we still have ownership over that data. So that's something like when I talk to my parents, you know, they're concerned about privacy. You know, they're concerned about interacting with AI and this, you know, just like there's always been concerns about data privacy, but only amplified, you know, as we're interacting with these types of agents, whether that's visual information or a large language model, but we really want to make it so that people can interact with artificial intelligence in a way that feels safe and that feels secure. And I think that's fundamentally tied to having that embedded in machines that they own. And so they can retain the sovereignty over the data about themselves. And it doesn't just become aggregated and owned by a much larger corporation. So that's, that's one thing when I'm looking forward, I think, you know, a lot of what I talked about there were some consumer devices, but there's also vast, vast opportunity in industrial automation. You know, we're going to see the automation of obviously we're seeing autonomous vehicles, like there are taxis riding all around San Francisco, but those business models are economical today because of cost of compu"}, "podcast_summary": "In this podcast episode, the host interviews Gordon Wilson, the CEO of RAIN AI, a company focused on building integrated hardware and software platforms for affordable AI solutions. Wilson explains that RAIN's mission is to make AI cheaper by reimagining the hardware and algorithms that support AI models. Their first product is an edge AI processor that is expected to be 100 times more efficient than current processors on the market. Wilson emphasizes that RAIN's products are built for integrated inference and training, allowing machines to not only make decisions but also learn and adapt on their own. He discusses the importance of edge AI, which refers to deploying AI models to make decisions on the fly at the location where information is received. This is in contrast to cloud-based AI, which is expensive and not suitable for mass deployment. Wilson provides examples of the problems that RAIN's edge AI solutions can solve, including making drones and robots more intelligent and capable of adapting to changing conditions, enabling faster and more reliable gesture recognition and eye tracking in augmented reality and virtual reality applications, and embedding intelligence into every mobile phone to unlock various use cases, such as object recognition, extended reality experiences, and advanced security protocols. He also mentions the importance of privacy and security in interacting with AI, ensuring that individuals retain sovereignty over their data. Wilson sees vast opportunities in industrial automation and predicts a future of more responsive and personalized AI experiences.", "podcast_guest": "Gordon Wilson", "podcast_guest_org": "RAIN", "podcast_guest_title": null, "podcast_guest_info": "Not Available", "podcast_guest_url": null, "podcast_highlights": "\"RAIN AI is focused on solving the most fundamental problems in artificial intelligence by making both inference and training dramatically cheaper. Our first products will be edge AI processors that are about 100x more efficient than the incumbent processors. We want to embed intelligence into every type of machine, whether it's drones, robots, augmented reality and virtual reality systems, or mobile phones. We aim to make AI more responsive, personalized, and secure for everyone.\" - Gordon Wilson, CEO of RAIN AI.\n\n(Source: \"Beyond GPU: Edge AI Computing Challenges and Solutions\" on the AI in Business podcast)", "podcast_hashtags": ["podcast", "AI", "edge", "AI", "computing", "hardware", "platform", "business", "problems"]}